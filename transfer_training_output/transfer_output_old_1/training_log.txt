Model: FineTuneModel(
  (base_model): ImageJacobianNet(
    (layers): ModuleList(
      (0): Linear(in_features=6, out_features=64, bias=True)
      (1): Linear(in_features=64, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
      (3): Linear(in_features=64, out_features=12, bias=True)
    )
    (norms): ModuleList(
      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (relu): LeakyReLU(negative_slope=0.01)
    (dropout): Dropout(p=0, inplace=False)
  )
  (new_layer_list): ModuleList(
    (0): Linear(in_features=12, out_features=64, bias=True)
    (1): Linear(in_features=64, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=12, bias=True)
  )
  (batch_norm_list): ModuleList(
    (0-1): 2 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (relu): LeakyReLU(negative_slope=0.01)
  (dropout): Dropout(p=0, inplace=False)
)
Base Model path: training_new/training_output_new_29/final_model.pt
Dataset path: cleaned_data/new_cleaned_data_10.csv
Number of epochs: 200
Batch size: 128
Learning rate: 0.005
Loss function: L1Loss()
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x77aa4ff87640>
