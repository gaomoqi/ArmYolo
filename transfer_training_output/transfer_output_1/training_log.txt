Model: FineTuneModel(
  (layers): ModuleList(
    (0): Linear(in_features=6, out_features=64, bias=True)
    (1): Linear(in_features=64, out_features=128, bias=True)
    (2): Linear(in_features=128, out_features=60, bias=True)
  )
  (bn_layers): ModuleList(
    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (dropout_layers): ModuleList(
    (0-2): 3 x Dropout(p=0.5, inplace=False)
  )
  (fc4): Linear(in_features=60, out_features=4, bias=False)
  (leaky_relu): LeakyReLU(negative_slope=0.01)
)
Base Model path: training_new_output/training_output_new_4/final_model.pt
Dataset path: cleaned_data/new_cleaned_data_11.csv
Number of epochs: 200
Batch size: 64
Learning rate: 0.01
Loss function: L1Loss()
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0
)
Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x764868030c70>
